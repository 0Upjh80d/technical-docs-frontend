{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Analytics &amp; AI Documentation","text":"<p>Welcome to the Data Analytics &amp; AI Documentation page of Synapxe, your hub for all technical resources, guidelines, and best practices. As a pioneering health tech company, we are committed to leveraging the power of data analytics and artificial intelligence to revolutionize the healthcare industry, enhance patient outcomes, and drive innovation.</p>"},{"location":"#our-mission","title":"Our Mission","text":"<p>At [Your Company Name], our mission is to harness cutting-edge technology to provide innovative solutions that improve health and wellbeing. We believe in the transformative potential of data and AI to deliver personalized, efficient, and effective healthcare services.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<ul> <li>Comprehensive Guides: Step-by-step tutorials and detailed guides to help you navigate our data analytics and AI platforms.</li> <li>API Documentation: In-depth documentation for our APIs, enabling seamless integration and utilization of our tools.</li> <li>Best Practices: Recommendations and best practices for implementing data analytics and AI in healthcare settings.</li> <li>Case Studies: Real-world examples of how our technology is being used to make a difference in healthcare.</li> <li>Technical Support: Resources and contact information for technical assistance and support.</li> </ul>"},{"location":"#why-data-analytics-ai-in-healthcare","title":"Why Data Analytics &amp; AI in Healthcare?","text":"<p>The integration of data analytics and AI in healthcare offers immense opportunities to enhance patient care, streamline operations, and enable predictive insights. From personalized treatment plans to early disease detection, our technology empowers healthcare providers to make data-driven decisions that can save lives and improve health outcomes.</p>"},{"location":"#join-us-in-transforming-healthcare","title":"Join Us in Transforming Healthcare","text":"<p>We invite healthcare professionals, data scientists, developers, and innovators to explore our documentation and join us on our journey to transform healthcare through data and AI. Whether you're integrating our APIs, developing new applications, or seeking to understand how AI can be applied to healthcare, you'll find the resources you need right here.</p>"},{"location":"blog/","title":"Articles","text":""},{"location":"blog/2024/08/26/best-practices-in-retrieval-augmented-generation/","title":"Best Practices in Retrieval-Augmented Generation","text":"<p>Generative Large Language Models (LLMs) are prone to generating outdated information or fabricating facts. Retrieval-Augmented Generation (RAG) techniques combine the strengths of pre-training and retrieval-based models to mitigate these issues, enhancing model performance.</p> <p>By integrating relevant, up-to-date information, RAG improves the accuracy and reliability of responses. Additionally, RAG enables rapid deployment of applications without the need to update model parameters, provided that query-related documents are available.</p> <p>This article delves into optimal practices for RAG, aiming to balance performance and efficiency.</p> Retrieval-augmented generation workflow."},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/","title":"Sentence Transformers for Sentence Similarity","text":"<p>In this article, we will take a look at the history leading up to the creation of Sentence Transformers, the shortcomings of past architectures across various Natural Language Processing (NLP) tasks (mainly sentence similarity) and how Sentence Transformers tackle these problems.</p>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#introduction","title":"Introduction","text":"Overview history of Sentence Transformer."},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#recurrent-networks","title":"Recurrent Networks","text":"From left to right: Vector-to-sequence, Sequence-to-vector and Sequence-to-sequence. <p>Clearly, Recurrent Neural Networks (RNNs) are versatile but for language problems, they have their disadvantages:</p> <p>Disadvantages:</p> <ol> <li>Slow to train and slow at inference</li> <li>This is because the input words are processed one at a time, sequentially. Therefore, longer      sentences just take a longer time.</li> <li>Do not truly understand context</li> <li>RNNs only learn about a word based on the words that came before it. In reality, the context of      a word depends on the sentence as a whole.</li> <li>Bidirectional Long Short-Term Memory (LSTMs) try to address this but even here, the left to      right and right to left context are learned separately and are concatenated so some of the true      context are lost.</li> </ol>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#transformer-networks","title":"Transformer Networks","text":"The encoder-decoder structure of the Transformer architecture. Taken from \u201cAttention Is All You Need\u201c. A sentence passing through the Transformer generating an embedding vector for each word. <p>For English to French translation, we pass in the entire English sentence into the encoder simultaneously. Then, we get the corresponding word vectors simultaneously. These word vectors encode the meaning of the word and they are better than RNNs because they understand bidirectional context through attention units.</p> <p>Now we pass these vectors into the decoder along with the previously generated French words to generate the next French word in the sentence. We keep passing the French words that were generated into the decoder until we hit the end of sentence.</p> <p>Transformers work well for sequence to sequence problems but for the specific natural language problems like question answering and text summarization, even Transformers have drawbacks related to one fact \u2014 language is complicated.</p> <p>Disadvantages:</p> <ol> <li>Need a lot of data</li> <li>Architecture may not be complex enough</li> <li>Transformers may not be complex enough to understand patterns to solve these language problems.      After all, Transformers weren\u2019t designed to be language models so the word representations      generated can still be improved.</li> </ol>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#bert-networks","title":"BERT Networks","text":"<p>BERT was introduced to extend the capabilities of the Transformer. BERT was built with the ideology that different Natural Language Processing (NLP) problems all rely on the same fundamental understanding of language.</p> Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers)."},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#phases","title":"Phases","text":"<p>BERT undergoes two phases of training:</p> <ol> <li>Pre-Training: Understand Language</li> <li>Fine Tuning: Understand Language specific tasks</li> </ol>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#advantages-over-transformers","title":"Advantages over Transformers","text":"<ol> <li>Needing a lot of data \u2192 Fine tuning does not require obscene amounts of data</li> <li>Architecture may not be complex enough \u2192 BERT is a stack of Transformer encoders and is    therefore known as B\u200bidirectional E\u200bncoder R\u200bepresentations from    T\u200bransformers.</li> <li>Bidirectional: It is bidirectional since it understands the context of words looking both      ways via attention.</li> <li>Encoder &amp; Transformers: Since BERT is essentially a stack of the encoder part of the      Transformer.</li> <li>Representations: Since BERT is pre-trained to be a language model, it better understands      word representations. This means the output word vectors from BERT better encapsulates the      meaning of the words in sentences.</li> </ol> Various NLP tasks. <p>The big takeaway here is that BERT can now solve a host of complex language specific problems except for one type.</p> <p> !!! example \"\ud83d\udcad Imagine\"</p> <pre><code>Imagine you\u2019re a Data Scientist at Quora which is a question answer site and you want to design a system that find related questions to the one that is currently being asked. How would we solve this with BERT?\n\n&lt;figure markdown=\"span\"&gt;\n  ![quora](../../assets/images/quora.png){ width=500 }\n  &lt;figcaption&gt;You're a Data Scientist trying to design a system that find related questions to the one that is currently being asked&lt;/figcaption&gt;\n&lt;/figure&gt;\n</code></pre> <p>Goal: Determine questions similar to the one being asked. </p> A simple schematic diagram illustrating the process of embedding the words across many sentences. <p>Steps:</p> <ol> <li>First take the question that is being asked and another question that had been asked in the past,    pass both of these questions into BERT</li> <li>BERT generates word vectors</li> <li>Pipe these word vectors into some feed forward layer such that the output would be a single    neuron corresponding to the similarity score</li> <li>Repeat the steps for every question on the platform to compute the pairwise similarity</li> <li>Select the highest similarity scores and the corresponding questions will be the most similar and    relevant to the question that is being asked</li> </ol> <p> !!! warning</p> <pre><code>However, there is a big issue here. If there are 100 million questions on the platform, we\u2019d have to run the forward pass of BERT 100 million times every single time a new question comes in. This is not viable!\n</code></pre> <p>So the next question so ask is: how do we make BERT work for the current goal?</p>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#sentence-transformers","title":"Sentence Transformers","text":""},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#pass-1-high-level-idea","title":"Pass 1: High Level Idea","text":"Embedding space: It contains vectors of the questions that represent meaning. <p>Steps:</p> <ol> <li>We would want to pass the new question into BERT to get a single vector that represents the    meaning of the question.</li> <li>Compare the vector of the new question to the vectors of all other questions using a similarity    metric (i.e. cosine similarity).</li> <li>Return the nearest neighbours as the most related questions to the new question.</li> </ol> <p>Therefore, for every new question asked, we only require a single forward pass of the BERT model not 100 million times as mentioned before. This is great because computing simple similarity metrics between vectors is much cheaper than passing in all questions on the platform through the complex model every time you need to make a decision.</p>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#pass-2-sentence-transformers","title":"Pass 2: Sentence Transformers","text":"<p>In the first pass, a new question is passed into BERT to get a single vector that represents the question. However, BERT only gives us word vectors. Therefore, in order to get a single vector, you\u2019ll need to somehow aggregate these word vectors by passing it through some unit.</p> <p>The most straightforward way of doing this is to take the average of these vectors. This is known as mean pooling. Another way is to take the maximum value across every dimension of the embedding. This is known as max pooling.</p> BERT outputs vectors for each word so in order to get a vector for the question/sentence, we need to aggregate these word vectors. This is the simplest form of a Sentence Transformer. <p>The diagram above shows the simplest form of a Sentence Transformer but the output vector generated is extremely poor quality. Its quality is so poor that you might be better off simply taking the average of GloVe embeddings (and not even using BERT).</p> Source: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks https://arxiv.org/pdf/1908.10084 <p>How to get sentence vectors with meaning?</p> <p>In order for BERT to create sentence vectors that actually have meaning, we need to further train it (fine-tune) on sentence level tasks (refer to next section for more information).</p> <p>Once we train (fine-tune) BERT on one or all of these tasks, the sentence vector generated becomes a good representation of the sentence \u2014 that is, it encodes the meaning of the sentence very well.</p> <p>This is important since it means that closer the vectors are in terms of distance, the more similar is the meaning.</p> <p>Info</p> <p>In our Quora questioning setting, we would pass every question through the sentence transformer once and store them somewhere for future use. Then when a new question comes in, we pass only that question through the sentence transformer to get the sentence vector representation and then determine the questions with the highest cosine similarity and surface them as related questions. We can find the nearest neighbours through some nearest neighbours techniques:</p> <ul> <li>ANNOY (Approximate Nearest Neighbours)</li> <li>KNN Elastic Search</li> </ul>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#pass-3-sentence-transformers-training","title":"Pass 3: Sentence Transformers Training","text":"<p>BERT is good at word representations but we want to make a Sentence Transformer that is good with sentence representations. To do this, we fine-tune BERT on any or all of the three sentence related tasks:</p> <ol> <li>Natural Language Inference (NLI)</li> <li>Sentence Text Similarity (STS)</li> <li>Triplet Dataset</li> </ol>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#natural-language-inference-nli","title":"Natural Language Inference (NLI)","text":"Natural Language Inference: Does sentence 1 entails or contradicts sentence 2? If neither, the \u201cneutral\u201d category will should be predicted. <p>NLI is a task that takes in two sentences and determines if sentence 1 entails or contradicts sentence 2 or simply neither. See some examples below:</p> <p>Examples</p> <p>Entailment</p> <ul> <li>Sentence 1: \u201cSay hello to me!\u201d</li> <li>Sentence 2: \u201cGreet me!\u201d</li> </ul> <p>Neutral</p> <ul> <li>Sentence 1: \u201cSay hello to me!\u201d</li> <li>Sentence 2: \u201cTwo people greeting and playing together.\u201d</li> </ul> <p>Contradiction</p> <ul> <li>Sentence 1: \u201cSay hello to me!\u201d</li> <li>Sentence 2: \u201cYou\u2019re ignoring me!\u201d</li> </ul> <p>This allows BERT to understand sentence meanings as a whole. For training NLI, a Siamese network is used. \u201cSiamese\u201d means twins so we have two of the exact same Sentence Transformer networks connected in this fashion.</p> A Siamese Network. <p>If we want to compare two sentences, we pass them through the different BERT networks to get word representations. These word vectors are then combined to create a sentence vector and then concatenate the two sentence vectors and their difference. The output is a softmax classification which can be one of these three classes \u2014 entailment, contradiction or neutral.</p> Concatenating the two sentence vectors and their difference. <p>Note</p> <p>Note that the mean pooling and concatenation look really arbitrary but they were chosen because they yielded the best results than any other strategy \u2014 like choosing max pooling or simply only considering the absolute difference between the vectors instead.</p> <p>During inference time, we only need the Sentence Transformer piece where we get a question and then we get the corresponding sentence vector. This vector is the sentence representation that encodes the meaning of the sentence (very well, hopefully).</p>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#sentence-text-similarity-sts","title":"Sentence Text Similarity (STS)","text":"<p>Another task we can use to fine-tune BERT to understand sentences is using STS. Given two sentences, output the score of how similar they are.</p> Sentence Text Similarity: How similar is sentence 1 to sentence 2? <p>Just like NLI, this is also trained with a Siamese network. During training, we pass the two sentences to compare through different Sentence Transformers to get these sentence vectors and then compute the cosine similarity between these sentence vectors to get the a value between \\(-1\\) and \\(1\\). These are then compared to an actual labelled similarity rating on a scale of \\(1\\) to \\(5\\) which is normalized to be comparable to the output score. We minimize the squared difference between the two so that the model can be trained.</p> Consine similarity between two sentences."},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#triplet-dataset","title":"Triplet Dataset","text":"<p>A third type of task that we can train Sentence Transformers is using a dataset that has triple of sentences. The main sentence is called the \u201canchor\u201d, the next sentence is a sentence that is \u201crelated\u201d and the last sentence being one that is \u201cunrelated\u201d to the \u201canchor\u201d.</p> Triplet Dataset: One sentence is the anchor, another is a related sentence to the anchor and the other is unrelated. <p>We can quickly make this type of dataset by picking a Wikipedia page, then choosing a sentence to be the \u201canchor\u201d and the next sentence in the same paragraph can be chosen as the \u201crelated\u201d sentence and then choose a sentence from another paragraph as the \u201cunrelated\u201d sentence. See screenshot below for an example.</p> Yellow sentence being the anchor. Blue sentence being the related sentence and pink sentence being the unrelated sentence. <p>The network is a triplet (not siamese, or twins) of the exact same Sentence Transformer architectures. During training, we pass each sentence through a Sentence Transformer to get three sentence vectors; \\(S_{a}\\), \\(S_{+}\\) and \\(S_{-}\\).</p> <p>We want to make sure the distance between the anchor and the related sentence is small and the distance between the anchor and unrelated sentence is large. This is so that the meanings are learned.</p> Loss to minimize in the Triplet dataset."},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#conclusion","title":"Conclusion","text":"<p>Regardless which of the tasks is chosen for training the Sentence Transformers, during inference time, we should be able to pass in some sentences and generate sentence representation vectors that encode the meaning of the sentences very well.</p>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#pass-4-sentence-transformers-inference","title":"Pass 4: Sentence Transformers Inference","text":"<p>Going back to our Data Science job at Quora, how do we recommend similar questions? Before additional questions are asked, we want to pass in every single question/sentence through the fine-tuned Sentence Transformer to get the corresponding sentence vectors. These vectors are good sentence representations (if fine tuning did not go wrong). These vectors all live in a space also known as the embedding space as previously seen above.</p> <p>Next, when a new question comes in, we pass it through our Sentence Transformer to get the sentence representation or sentence embedding. Next, we determine the cosine similarity between the new question and every other candidate question. Finally, we will return the closest questions list as the related questions.</p> <p>For small datasets, we can determine the cosine similarity for a new question with every other question but it becomes increasingly harder to do when there are hundreds and millions of questions (very common especially on a platform like Quora).</p> Quora Statistics 2024. <p>To solve this issue, there are a couple of algorithms we can use. Spotify uses an Approximate Nearest Neighbours algorithm called ANNOY to recommend music to you. In this case, songs are embedded into vectors.</p> <p>Another way to quickly compute the nearest neighbours is through AWS which has an extremely efficient implementation of the k-Nearest Neighbours algorithm.</p>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#summary","title":"Summary","text":""},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":"<p>Advantages:</p> <ul> <li>Able to deal with Sequence-to-Sequence problems</li> </ul> <p>Disadvantages:</p> <ul> <li>Slow to train and during inference</li> <li>Do not truly understand context</li> </ul>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#transformers","title":"Transformers","text":"<p>Advantages:</p> <ul> <li>Replace Recurrent units with Attention units, addressing past concerns</li> <li>Solves Sequence-to-Sequence problems</li> </ul> <p>Disadvantages:</p> <ul> <li>Not necessarily complex enough to understand language</li> </ul>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#bert","title":"BERT","text":"<p>Advantages:</p> <ul> <li>Stack of Transformer encoders</li> <li>Complex enough to solve a host of NLP problems</li> </ul> <p>Disadvantages:</p> <ul> <li>Not good with sentence similarity tasks</li> </ul>"},{"location":"blog/2024/06/10/sentence-transformers-for-sentence-similarity/#sentence-bert","title":"Sentence BERT","text":"<p>Advantages:</p> <ul> <li>Fine tunes BERT on Sentence Similarity Tasks, addressing past concerns</li> </ul>"},{"location":"home/contribution_guides/classes_types_and_all/","title":"Classes, Types, and All","text":"<p>This page is a overview on how objects should be documented.</p> <p>A special syntax should be adhered to when labeling objects, attributes, etc. within documentation to improve readability. These special labels are called <code>doculabels</code>.</p> <pre><code>@\u200blabel(&lt;label&gt;)\n</code></pre> <p>Zero width space character</p> <p>To prevent any of the strings in this document from being redered as a <code>doculabel</code>, a zero space character was placed into the string. Do not copy the code in any of the blocks.</p> <p>The following labels can be used, simply type any of the texts below into the rounded brackets (and remove the angled brackets).</p> <ul> <li>@label(class)</li> <li>@label(pipe)</li> <li>@label(service)</li> <li>@label(interface)</li> <li>@label(type)</li> <li>@label(attr)</li> <li>@label(meth)</li> <li>@label(func)</li> <li>@label(private)</li> <li>@label(read only)</li> <li>@label(deprecated)</li> </ul> <p>While they can be rendered anywhere on the page, they should only be used in header elements (H2 and below).</p>"},{"location":"home/contribution_guides/classes_types_and_all/#page-layout","title":"Page Layout","text":"<p>Objects/types/interfaces/etc. should always have the second highest hierarchy, and should never be the highest. Page titles (<code>#</code> H1 tags) should be reserved purely for the name of the page.</p> <p>All objects and attributes (including private, read only, deprecated) should be documented.</p> <p>Below is a very rough example of how a page should be written.</p> Markdown <pre><code>## @\u200blabel(class) MyExampleClass\n\nShort description of my class.\n\n### Attributes\n\n#### @\u200blabel(attr) codeAttributeName\n`attrTypr` and explain what this attribute is for\n\n### Methods\n\n#### @\u200blabel(meth) My Humanised Method Name\n\n    ```\n    myMethodName(arg0:type):type\n    ```\n\nDescription\n: Some verbose reason what this method is for.\n\nParameters\n: `arg0` (`type`): Description of `arg0` and its relation to the method.\n\nReturns\n: `type` and its significance.\n</code></pre> <p>Generally speaking, the following patterns can be followed.</p> <ul> <li>Descriptive sections (e.g. attributes, methods, specific method groups) should be <code>H3</code>.</li> <li>Actual code attributes, methods, and all should be <code>H4</code>.</li> </ul> <p>Note</p> <p>See the example below for a visual example of how a page should look like.</p>"},{"location":"home/contribution_guides/classes_types_and_all/#attributes","title":"Attributes\u200b","text":"<p>All attributes must be labeled, with their respective types. To mark additional <code>doculabels</code>, they must be done in this order.</p> <ol> <li>@label(deprecated)</li> <li>@label(private)</li> <li>@label(read only)</li> <li>@label(attr)</li> </ol> Markdown <pre><code>#### @\u200blabel(attr) myAttribute\n`type` example to show attribute\n\n#### @\u200blabel(private) @\u200blabel(attr) youShouldntUseThisOutside\n`type` an example private attribute\n</code></pre>"},{"location":"home/contribution_guides/classes_types_and_all/#methods-and-functions","title":"Methods and Functions","text":"<p>Methods can be grouped together under the same <code>H3</code> header if they have similar relations.</p> Markdown <pre><code>### Methods related to Apples\n\n#### @\u200blabel(meth) Create apple\n...\n\n#### @\u200blabel(meth) Delete apple\n...\n\n### Methods related to Oranges\n\n#### @\u200blabel(meth) Create orange\n...\n\n#### @\u200blabel(meth) Delete orange\n...\n</code></pre> <p>For each method or function, the following items must be documented for clarity.</p> <ol> <li>Call signature</li> <li>Description</li> <li>Parameters</li> <li>Return type</li> </ol>"},{"location":"home/contribution_guides/classes_types_and_all/#call-signature","title":"Call Signature","text":"<p>This should be documented in a code block in the language the piece of code was written in. If it was written in TypeScript, it should look like this.</p> <pre><code>async function createApple(apple:Apple):Promise&lt;string&gt;\n</code></pre>"},{"location":"home/contribution_guides/classes_types_and_all/#description","title":"Description","text":"<p>A short sentence should be added to give a very summarised brief on what the function or method should do.</p> Markdown <pre><code>Description\n: Creates an apple in the fruit basket.\n</code></pre>"},{"location":"home/contribution_guides/classes_types_and_all/#parameters","title":"Parameters","text":"<p>For each of the parameters, the name, type, and description of it should be given.</p> <p>The syntax should look like this.</p> <pre><code>Parameter\n: `nameOfParam1` (`type`): Description\n: `nameOfParam2` (`type`): Description\n</code></pre> Markdown <pre><code>Parameters\n: `apple` (`Apple`): Creates the `Apple` fruit that should be added to the basket.\n</code></pre>"},{"location":"home/contribution_guides/classes_types_and_all/#returns","title":"Returns","text":"<p>This is particularly important as it outlines what the return type is, and its significance.</p> Markdown <pre><code>Returns\n: `string` Id of the apple created\n</code></pre>"},{"location":"home/contribution_guides/classes_types_and_all/#labelclass-examplefruitbasket","title":"@label(class) ExampleFruitBasket","text":"<p>This is an example of how a rendered class would look like</p>"},{"location":"home/contribution_guides/classes_types_and_all/#attributes_1","title":"Attributes","text":""},{"location":"home/contribution_guides/classes_types_and_all/#labelattr-fruits","title":"@label(attr) fruits","text":"<p><code>Fruit[]</code> an array containing all the fruits in the basket.</p>"},{"location":"home/contribution_guides/classes_types_and_all/#methods-related-to-apples","title":"Methods related to apples","text":""},{"location":"home/contribution_guides/classes_types_and_all/#labelmeth-add-apple","title":"@label(meth) Add apple","text":"<pre><code>addApple(apple:Apple):void\n</code></pre> Description Adds an apple to the fruit basket. Parameters <code>apple</code> (<code>Apple</code>): Apple to be added Returns <code>void</code>"},{"location":"home/contribution_guides/classes_types_and_all/#methods-related-to-oranges","title":"Methods related to oranges","text":""},{"location":"home/contribution_guides/classes_types_and_all/#labelmeth-add-orange","title":"@label(meth) Add Orange","text":"<pre><code>addOrange(orange:Orange):void\n</code></pre> Description Adds an orange to the fruit basket. Parameters <code>orange</code> (<code>Orange</code>): Orange to be added Returns <code>void</code>"},{"location":"home/contribution_guides/classes_types_and_all/#labelmeth-eat-orange-by-id","title":"@label(meth) Eat Orange by Id","text":"<pre><code>async eatOrange(id:string, round:boolean):Promise&lt;number&gt;\n</code></pre> Description Eat and orange and count the number of bites. Parameters <code>id</code> (<code>string</code>): UUID of the <code>Orange</code> to be eaten. <code>round</code> (<code>boolean</code>): Specifies if the <code>Orange</code> must be round. Returns <code>number</code> of bites needed to finish eating the orange"},{"location":"home/contribution_guides/endpoints/","title":"Endpoints","text":"<p>Endpoints are documented with OpenAPI standards.</p> <p>Plugin reference</p> <p>Documentation for the plugin used to render the OpenAPI documentation can be found here.</p>"},{"location":"home/contribution_guides/formatting/","title":"Formatting","text":""},{"location":"home/contribution_guides/formatting/#frontmatter","title":"Frontmatter","text":"<p>Frontmatter is the \"metadata\" section of each markdown page. The following attributes must be present on all pages.</p> <ul> <li><code>updated</code> -- date of which the page was last updated, in the DD-mmm-YYYY format.</li> <li><code>authors</code> -- a string, or a list of strings matching the authors name(s) that contributed to the page.</li> </ul> <p>These are required as the information is used to render the <code>updated</code> and <code>author</code> information at the bottom of the pages (hint: see the bottom of this page).</p> <p>The following code block is an example of how to write the frontmatter at the very top of each markdown file.</p> <pre><code>---\nupdated: 23 July 2024\nauthors: Johnny Tan\n---\n</code></pre> <p>Author attribution</p> <p>The authors.py hook was used in the rendering process of authors.</p>"},{"location":"home/contribution_guides/formatting/#hierarchy-of-headers","title":"Hierarchy of Headers","text":"<p>Headers should reflect the logical hierarchy of the content. They must be used in sequential order, starting from H1 for the main title, then H2 for major subsections, followed by H3 for subtopics within those subsections, and so on. This structure is crucial as it helps in organizing the content clearly and logically, making it easier for readers to follow and understand the flow of information.</p>"},{"location":"home/contribution_guides/introduction/","title":"Introduction","text":"<p>Documentation is crucial as it offers a comprehensive guide for developers to comprehend and maintain software. This is equally vital for newly onboarded developers as well as for long-standing team members. Furthermore, as the codebase grows in size and complexity, the documentation serves as a quick reference for team members.</p>"},{"location":"home/contribution_guides/introduction/#guides","title":"Guides","text":"<ul> <li>Formatting -- Refer to this section for our standard markdown formatting guidelines applicable across all documentation.</li> <li>Endpoints -- Details on how to document endpoints.</li> <li>Classes, types, and all -- This section offers detailed guidelines on preparing technical documentation for various objects and types within our codebase. It serves as a critical resource for accurately and precisely describing the structure and functionalities of our software.</li> </ul>"},{"location":"projects/genai/conversational-assistant/webapp/backend/api/endpoints/","title":"Endpoints","text":"<p>For testing</p> <p>Run the webapp locally first before testing the endpoint</p> <p></p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/","title":"Introduction","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/#high-level-dependency","title":"High Level Dependency","text":"<p>The frontend for HealthierME 2.0 is build using Angular v18.</p> Domain Technology Remarks Framework Angular v18 Modularity between components and business logic. Component PrimeNG Large range of components out of the box. Styling TailwindCSS Fast tooling for styling of components. Icons Lucide One of the largest icon library with native support for Angular."},{"location":"projects/genai/conversational-assistant/webapp/frontend/#philosophy","title":"Philosophy","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/#data-persistence","title":"Data Persistence","text":"<p>To minimise the technical complexity of maintaining states, all stateful logic is handled on the frontend, allowing the backend to be purely stateless. As such, <code>LocalStorage</code> and <code>IndexedDb</code> stores are used to persist user information. This strategy allows us to make do without an authentication (or user based) system, drastically reducing technical complexity -- particularly important given the short turn around time given to produce a proof-of-concept.</p> <p><code>Message</code>s are persisted in the browser using <code>IndexedDb</code> and are queried by the ID of the profile that is currently being interacted with. This is managed by ChatMessageService.</p> <p>To manage user preferences, <code>LocalStorage</code> was used, and is managed by PreferenceService. Preferences include the chat mode, and Voice Activity Detection (VAD) settings.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/#software-architecture","title":"Software Architecture","text":"<p>The diagram below shows a very, very, high level overview on how the services and components interact with each other.</p> <p>Warning</p> <p>This diagram does not cover every relation and interaction that has been implemented. It does however give a broad idea of the intention behind the implemented structure and how its abstraction of business logic can be expanded.</p> <p>The implemented logic is more nuanced than depicted, with more components and a few more services.</p> <p>Services can be grouped into two general types, a service to abstract out business logic, Abstraction Service Layer (ASL), and services that are responsible for actionable items, Service Layer (SL).</p> <pre><code>flowchart RL\n    subgraph ui [UI]\n        direction RL\n        voice[[Voice Component]]\n        text[[Text Input Component]]\n    end\n\n    subgraph asl [Abstraction Service Layer]\n        direction RL\n        convoBroker{{Conversation Broker}}\n    end\n\n    subgraph sl [Service Layer]\n        direction RL\n        endpoint([Endpoint Service])\n        vad([Voice Activity Detection Service])\n        chat([Chat Message Service])\n        audio([Audio Service])\n        audioPlayer([Audio Player Service])\n    end\n\n    ui --&gt; asl\n    asl --&gt; sl</code></pre> <p>The ultimate goal of this layered structure was to abstract away business logic from components.</p> <p>For example, to process a voice input:</p> <ol> <li>Voice recording must be started (with or without VAD)</li> <li>An API call must be made</li> <li>The transcribed user message must be persisted</li> <li>The streamed API response must be parsed</li> <li>LLM response must be persisted as a message</li> <li>Audio of the LLM response must be played</li> </ol> <p>By adopting the UI/ASL/SL layer structure, the logic is consolidated into ASL services that can be accessed from other components, encouraging reusability of the business logic and keeping UI components readable and simple to maintain.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/","title":"Audio Player Service","text":"<p>This service is responsible for playing all sounds on the frontend. This includes:</p> <ul> <li>LLM audio response (voice chat)</li> <li>Text to speech</li> </ul>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelservice-audioplayerservice","title":"@label(service) AudioPlayerService","text":"<p>To facilitate audio streaming from LLM voice responses, it implements an audio queue system. Audio blobs are played in the sequence they are added in. This implementation was due to how audio files of the LLM response was received on the frontend.</p> <p>Assumption</p> <p>The current implementation generally assumes that there will only be one source of audio, voice LLM and TTS included.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#attributes","title":"Attributes","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelprivate-labelattr-audioelement","title":"@label(private) @label(attr) audioElement","text":"<p><code>HTMLMediaElementWithCaptureStream</code> source of audio being played.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelprivate-labelattr-queue","title":"@label(private) @label(attr) queue","text":"<p><code>Blob[]</code> contains audio files to be played.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelattr-stream","title":"@label(attr) $stream","text":"<p><code>BehaviorSubject&lt;MediaStream|null&gt;</code> keeping track of the current audio stream. This will be consumed by the waveform visualiser.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelattr-playing","title":"@label(attr) $playing","text":"<p><code>BehaviorSubject&lt;boolean&gt;</code> to keep track of the current playing state.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelprivate-labelmeth-play-next-in-queue","title":"@label(private) @label(meth) Play Next in Queue","text":"<pre><code>private playNextInQueue(): void\n</code></pre> Description This method will play the next audio in the queue."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelmeth-get-audio-stream","title":"@label(meth) Get Audio Stream","text":"<pre><code>getAudioStream(): BehaviorSubject&lt;MediaStream|null&gt;\n</code></pre> Description Public method to retrieve the <code>BehaviorSubject</code> to track the current audio stream source. Returns <code>BehaviorSubject&lt;MediaStream|null&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelmeth-play","title":"@label(meth) Play","text":"<pre><code>play(...blob: Blob[]): void\n</code></pre> Description Method to add an audio file to the queue to be played. Parameters <code>...blob</code> (<code>...Blob[]</code>): Destructed array of <code>Blob</code>s to be played. Will be added to a queue."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelmeth-force-play","title":"@label(meth) Force Play","text":"<pre><code>forcePlayAndReplace(blob: Blob): void\n</code></pre> Description This method is to clear the current queue of audio files, and play the provided <code>Blob</code> audio. Parameters <code>blob</code> (<code>Blob</code>): Audio file to be played"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelmeth-stop-and-clear","title":"@label(meth) Stop and Clear","text":"<pre><code>stopAndClear(): void\n</code></pre> Description This method clears the current queue, stops the audio from being played and updates local states."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelmeth-play-start-voice-audio","title":"@label(meth) Play Start Voice Audio","text":"<pre><code>playStartVoiceAudio(): void\n</code></pre> Description This method plays a pre-defined audio file (<code>startvoice.mp3</code>) from the app's assets, used to signal the start of voice interaction."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-player/#labelmeth-play-stop-voice-audio","title":"@label(meth) Play Stop Voice Audio","text":"<pre><code>playStopVoiceAudio(): void\n</code></pre> Description This method plays a pre-defined audio file (<code>stopvoice.mp3</code>) from the app's assets, used to signal the end of voice interaction."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-service/","title":"Audio Service","text":"<p>This service is responsible for managing the microphone resources on the frontend.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-service/#labelservice-audioservice","title":"@label(service) AudioService","text":"<p>This service requests for the microphone resource when necessary during voice input, and releases the microphone resource after voice recording has ended.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-service/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-service/#labelprivate-labelmeth-get-mic-input","title":"@label(private) @label(meth) Get Mic Input","text":"<pre><code>async getMicInput(): Promise&lt;MediaStream&gt;\n</code></pre> Description <p>This method will retrieve the microphone resource from the device.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-service/#labelasync-labelmeth-stop-audio-tracks","title":"@label(async) @label(meth) Stop audio tracks","text":"<pre><code>async stopTracks(stream: MediaStream)\n</code></pre> Description <p>This method will release all audio tracks and release the microphone resource.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/audio-service/#labelasync-labelmeth-merge-audio-streams","title":"@label(async) @label(meth) Merge Audio Streams","text":"<pre><code>async mergeAudioStreams(...streams: MediaStream[]): Promise&lt;MediaStream&gt;\n</code></pre> Description <p>This method merges the audio streams into 1 audio stream.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/","title":"Chat Message Service","text":"<p>This service is meant to handle persistence of conversation.</p> <p>Foot gun</p> <p>The current implementation is a \"foot gun\" implementation that was easy to put together and work for the limited use cases that were laid out. However, there are inheirent drawbacks that might induce silent failures in the expected behavior.</p> <p>Currently, only one chat history (defined by <code>Profile.id</code>) can be tracked at any one point of time. Calling <code>ChatMessageService.load()</code> will drop the reference to previously loaded chat histories. This means that previous <code>BehaviorSubject</code>s tracking the conversation will be orphaned and will not recieve any updates upon changes.</p> <p>How to fix</p> <p>This can be improved by memoizing the loaded chat history into a <code>Record&lt;string, BehaviorSubject&lt;Message[]&gt;</code>, so all <code>BehaviorSubject</code>s can be tracked and updated as necessary. Calling <code>ChatMessageService.load()</code> can then return a reference to the memoized <code>BehaviorSubject</code>. This will allow multiple conversations to be loaded and updated simultaniously.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelservice-chatmessageservice","title":"@label(service) ChatMessageService","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#attributes","title":"Attributes","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelprivate-labelattr-messages","title":"@label(private) @label(attr) $messages","text":"<p><code>BehaviorSubject&lt;Message[]&gt;</code> tracking the currently active chat history.</p> <p>Important</p> <p>See the note above.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelprivate-labelattr-currentprofileid","title":"@label(private) @label(attr) $currentProfileId","text":"<p><code>BehaviorSubject&lt;string&gt;</code> to keep track of the currently loaded conversation. Mainly used to print a warning in console.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelmeth-load-messages","title":"@label(meth) Load Messages","text":"<pre><code>async load(profileId: string): Promise&lt;BehaviorSubject&lt;Message[]&gt;&gt;\n</code></pre> Description Method to load the conversation with a given profile into memory to be tracked for updates. Parameters <code>profileId</code> (<code>string</code>): Profile ID of the conversation to load. Returns <code>Promise&lt;BehaviorSubject&lt;Message[]&gt;&gt;</code> <p>Important</p> <p>See the note above.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelmeth-static-load-messages","title":"@label(meth) Static Load Messages","text":"<pre><code>async staticLoad(profileId: string): Promise&lt;Message[]&gt;\n</code></pre> Description Method to fetch the current existing messages with a given profile. This does not induce any side effects. Parameters <code>profileId</code> (<code>string</code>): ID of the profile to fetch for. Returns <code>Promise&lt;Message[]&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelmeth-insert-message","title":"@label(meth) Insert Message","text":"<pre><code>insert(message: Message): Promise&lt;void&gt;\n</code></pre> Description Method to persist a message in <code>IndexedDb</code>. Parameters <code>message</code> (<code>Message</code>): Message to be persisted Returns <code>Promise&lt;void&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/chat-message/#labelmeth-upsert-message","title":"@label(meth) Upsert Message","text":"<pre><code>upsert(message: Message): Promise&lt;void&gt;\n</code></pre> Description Method to update or create new message for persistence. Parameters <code>message</code> (<code>Message</code>): Message to be updated or created. Returns <code>Promise&lt;void&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/","title":"Conversation Broker Service","text":"<p>This service is responsible for managing interaction between UI components and other services related to chat (and voice) functionality.</p> <p>Isolation of business logic</p> <p>All interaction with voice and chat MUST come through this service. Logic related to all global interaction has been encapsulated into this service, so that UI components can focus on UI states and only UI related logic that responds to changes reflected in this service.</p> <p>This service should reflect the global state of events necessary for UI components.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelservice-convobrokerservice","title":"@label(service) ConvoBrokerService","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#attributes","title":"Attributes","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelattr-recorder","title":"@label(private) @label(attr) recorder","text":"<p><code>AudioRecorder</code> is the object responsible for recording user audio.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelattr-activeprofile","title":"@label(private) @label(attr) activeProfile","text":"<p><code>Profile | undefined</code> tracks the currently active profile. This is updated by a subscription instantiated by the constructor.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelattr-micstate","title":"@label(attr) $micState","text":"<p><code>BehaviorSubject&lt;MicState&gt;</code> is the global controller for the mic state, used to control the UI state of the main mic button on the voice page.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelattr-sendtimeout","title":"@label(attr) $sendTimeout","text":"<p><code>BehaviorSubject&lt;boolean&gt;</code> is a boolean flag used to signal if a timeout should be invoked.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelattr-iswaitingforvoiceapi","title":"@label(attr) $isWaitingForVoiceApi","text":"<p><code>BehaviorSubject&lt;boolean&gt;</code> to track if a response has been made from the backend API regarding voice chat.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#voice-chat-related-methods","title":"Voice Chat Related Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-initialise-voice-chat","title":"@label(private) @label(meth) Initialise Voice Chat","text":"<pre><code>private async initVoiceChat(): Promise&lt;void&gt;\n</code></pre> Description Method to initialise voice activity detection, and initialise <code>AudioRecorder</code>. It is called in the constructor of the service. This method instantiates the subscription for VAD detection, and contains side effects. <p>VAD is only active in voice mode</p> <p>To prevent undesired VAD, the initialised subscribtion to handle VAD will only trigger side effects when the user is in <code>ChatMode.Voice</code>.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-start-recording","title":"@label(private) @label(meth) Start Recording","text":"<pre><code>private handleStarteRecording(): void\n</code></pre> Description Method to trigger the start of audio recording."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-stop-recording","title":"@label(private) @label(meth) Stop Recording","text":"<pre><code>private handleStopRecording(): void\n</code></pre> Description Handle the stopping of recording, and post process the recorded audio. The recorded audio will be sent to the backend as a voice message."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-stop-audio-playback","title":"@label(private) @label(meth) Stop Audio Playback","text":"<pre><code>private handleStopPlaying(): void\n</code></pre> Description Method to stop the current audio playback and reset the mic state. Also unsubscribes from the voice stream to prevent further audio from playing."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-unsubscribe-from-voice-stream","title":"@label(private) @label(meth) Unsubscribe from Voice Stream","text":"<pre><code>private unsubscribeVoiceStream(): void\n</code></pre> Description Method to unsubscribe from the voice stream and reset any active voice subscriptions. This method ensures that any ongoing or pending voice stream is canceled properly to avoid memory leaks."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-play-base64-encoded-audio","title":"@label(private) @label(meth) Play Base64 Encoded Audio","text":"<pre><code>private async playAudioBase64(val:string): Promise&lt;void&gt;\n</code></pre> Description Method to play a base 64 encoded audio. This induces side effects in <code>AudioPlayerService</code>. Parameters <code>val</code> (<code>string</code>): Base 64 encoded audio file."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelprivate-labelmeth-send-voice","title":"@label(private) @label(meth) Send Voice","text":"<pre><code>private async sendVoice(audio: Blob, profile: Profile): Promise&lt;void&gt;\n</code></pre> Description Sends audio blob to the backend for LLM and voice chat functionality; it will handle the responses and updating of states related to voice chat. Side effects include audio player and chat message service. Parameters <code>audio</code> (<code>Blob</code>): Blob of audio recording file. <code>profile</code> (<code>Profile</code>): Profile used in the conversation"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelmeth-mic-button-click","title":"@label(meth) Mic Button Click","text":"<pre><code>handleMicButtonClick(): void\n</code></pre> Description Method used in callbacks when the mic button to trigger audio recording actions has been clicked."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#text-chat-related-methods","title":"Text Chat Related Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelmeth-send-chat","title":"@label(meth) Send Chat","text":"<pre><code>async sendChat(message: string, profile: Profile): Promise&lt;void&gt;\n</code></pre> Description Method to send a chat message. This will handle the persistence of messages into IndexedDB, and interacting with the backend via the <code>EndpointService</code>. Parameters <code>message</code> (<code>string</code>): User input message <code>profile</code> (<code>Profile</code>): Active profile used in the conversation."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#feedback-related-methods","title":"Feedback Related Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/conversation-broker/#labelmeth-send-feedback","title":"@label(meth) Send Feedback","text":"<pre><code>async sendFeedback(feedback: Feedback)\n</code></pre> Description Method to send a feedback object. This will handle the persistence of messages into IndexedDB, and interacting with the backend via the <code>EndpointService</code>. Parameters <code>feedback</code> (<code>feedback</code>): Feedback object"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/","title":"Endpoint Service","text":"<p>This service is used to interact with the backend. Understandably, this is the most crucial part in the context of backend frontend integration. Given that (at the point of writing) the endpoints were still evolving, this service served as a abstraction layer, so that the rest of the webapp will not break -- when API endpoints are updated, technically, this is the only service that needs to be updated, technically.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelservice-endpointservice","title":"@label(service) EndpointService","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#utility-methods","title":"Utility Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelmeth-text-to-speech","title":"@label(meth) Text to Speech","text":"<pre><code>async textToSpeech(text: string): Promise&lt;BehaviorSubject&lt;Blob | null&gt;&gt;\n</code></pre> Description Method to send text to the backend for conversion to speech. Parameters <code>text</code> (<code>string</code>): The text that needs to be converted to speech. Returns <code>Promise&lt;BehaviorSubject&lt;Blob | null&gt;&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelprivate-labelmeth-message-to-api-chat-history","title":"@label(private) @label(meth) Message to Api Chat History","text":"<pre><code>private messageToApiChatHistory(message: Message[]): ApiChatHistory[]\n</code></pre> Description Method to convert <code>Message</code> array into <code>ApiChatHistory</code> format for backend consumption. Parameters <code>message</code> (<code>Message[]</code>): Array of messages to convert. Returns <code>ApiChatHistory[]</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelprivate-labelmeth-message-to-api-chat-history-with-sources","title":"@label(private) @label(meth) Message to Api Chat History with Sources","text":"<pre><code>private messageToApiChatHistoryWithSources(message: Message[]): ApiChatHistorywithSources[]\n</code></pre> Description Method to convert <code>Message</code> array into <code>ApiChatHistorywithSources</code> format for backend consumption. Parameters <code>message</code> (<code>Message[]</code>): Array of messages to convert. Returns <code>ApiChatHistorywithSources[]</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelprivate-labelmeth-profile-to-api-profile","title":"@label(private) @label(meth) Profile to Api Profile","text":"<pre><code>private profileToApiProfile(profile: Profile): ApiProfile\n</code></pre> Description Method to convert <code>Profile</code> into <code>ApiProfile</code> format for backend consumption. Parameters <code>profile</code> (<code>Profile</code>) Returns <code>ApiProfile</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelmeth-send-voice","title":"@label(meth) Send Voice","text":"<pre><code>async sendVoice(\n    recording: Blob,\n    profile: Profile,\n    history: Message[]\n): Promise&lt;BehaviorSubject&lt;VoiceResponse|null&gt;&gt;\n</code></pre> Description Method to send voice recording to the backend. Parameters <code>recording</code> (<code>Blob</code>): Binary of file recording. <code>profile</code> (<code>Profile</code>): Active profile used in the conversation. <code>history</code> (<code>Message[]</code>): Chat history in the conversation. Returns <code>Promise&lt;BehaviorSubject&lt;VoiceResponse|null&gt;&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/endpoint/#labelmeth-send-chat","title":"@label(meth) Send Chat","text":"<pre><code>async sendChat(\n    message: Message,\n    profile: Profile,\n    history: Message[]\n): Promise&lt;BehaviorSubject&lt;ChatReponse|null&gt;&gt;\n</code></pre> Description Method to send chat (text) message to the backend for processing Parameters <code>message</code> (<code>Message</code>): Chat message from user. <code>profile</code> (<code>Profile</code>): Active profile used in the conversation. <code>history</code> (<code>Message[]</code>): Chat history in the conversation. Returns <code>Promise&lt;BehaviorSubject&lt;ChatResponse|null&gt;&gt;</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/","title":"Preference Service","text":"<p>This service handles the user preference across the webapp. It is responsible for persisting these states into local storage and listening to changes to local storage.</p> <p>Currently, this service is used for managing:</p> <ul> <li>Language</li> <li>Chat mode</li> </ul>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelservice-preferenceservice","title":"@label(service) PreferenceService","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#attributes","title":"Attributes","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelattr-chatmode","title":"@label(attr) $chatMode","text":"<p><code>BehaviorSubject&lt;ChatMode&gt;</code> is the state controller for the chat mode, which manages whether the app operates in text or voice mode.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelattr-language","title":"@label(attr) $language","text":"<p><code>BehaviorSubject&lt;Language&gt;</code> controls the preferred language for the application.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelprivate-labelmeth-initialise-preferences","title":"@label(private) @label(meth) Initialise Preferences","text":"<pre><code>private initialisePreferences(): void\n</code></pre> Description Initializes the preferences by subscribing to <code>BehaviorSubjects</code> and syncing them with local storage."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelprivate-labelmeth-listen-to-local-storage-events","title":"@label(private) @label(meth) Listen To Local Storage Events","text":"<pre><code>private listenToLocalStorageEvents(): void\n</code></pre> Description Listens for local storage changes and updates the corresponding preference states in real-time."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelprivate-labelmeth-load-from-local-storage","title":"@label(private) @label(meth) Load From Local Storage","text":"<pre><code>private loadFromLocalStorage&lt;T&gt;(key: PreferenceKey, defaultValue: T): T\n</code></pre> Description Loads a preference value from local storage, or uses the provided default value if no value is found in storage. Parameters <code>key</code> (<code>PreferenceKey</code>): The key under which the preference is stored. <code>defaultValue</code> (<code>T</code>): The default value to use if the preference is not found in storage. Returns <code>T</code>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelprivate-labelmeth-set-to-local-storage","title":"@label(private) @label(meth) Set To Local Storage","text":"<pre><code>private setToLocalStorage&lt;T&gt;(key: string, value: T): void\n</code></pre> Description Stores a preference value in local storage. Parameters <code>key</code> (<code>string</code>): The storage key for the preference. <code>value</code> (<code>T</code>): The value to store in local storage."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelmeth-set-chat-mode","title":"@label(meth) Set Chat Mode","text":"<pre><code>setChatMode(mode: ChatMode): void\n</code></pre> Description Sets the chat mode (either voice or text mode) for the application. Parameters <code>mode</code> (<code>ChatMode</code>): The chat mode to set, either <code>ChatMode.Voice</code> or <code>ChatMode.Text</code>."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#labelmeth-set-language","title":"@label(meth) Set Language","text":"<pre><code>setLanguage(language: Language): void\n</code></pre> Description Sets the preferred language for the application. Parameters <code>language</code> (<code>Language</code>): The selected language preference."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#older-implementations-labeldeprecated","title":"Older Implementations @label(deprecated)","text":"<p>Not Currently Used</p> <p>The following are older features previously handled by this service, but they are no longer in use. They remain documented here for reference but are not active in the current version of the app.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#show-live-transcription","title":"Show Live Transcription","text":"<p>Toggles live transcription of the user\u2019s voice input during interactions. This would show a live text transcription as the user speaks.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#voice-interrupt","title":"Voice Interrupt","text":"<p>Allows the user to enable or disable the ability to interrupt ongoing audio playback with a voice command.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#detect-voice-start","title":"Detect Voice Start","text":"<p>Controls whether the app would automatically detect when the user starts speaking to begin recording.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/preference/#detect-voice-end","title":"Detect Voice End","text":"<p>Controls the automatic detection of the end of the user\u2019s voice input, used to stop recording after a period of silence.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/","title":"Profile Service","text":"<p>This service is responsible for creating new profiles and managing existing profiles for users.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelservice-profileservice","title":"@label(service) ProfileService","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#attributes","title":"Attributes","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelprivate-labelattr-profiles","title":"@label(private) @label(attr) $profiles","text":"<p><code>BehaviorSubject&lt;Profile[]&gt;</code> stores all profiles retrieved from the database store.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelprivate-labelattr-currentprofileinurl","title":"@label(private) @label(attr) $currentProfileInUrl","text":"<p><code>BehaviorSubject&lt;string&gt;</code> tracks the current profile through the profile id in the url.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelmeth-set-profile-in-url","title":"@label(meth) Set Profile in URL","text":"<pre><code>setProfileInUrl(id: string)\n</code></pre> Description Method to update the existing url to the new profile linked to the argument 'id' passed into the function."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelmeth-create-profile","title":"@label(meth) Create Profile","text":"<pre><code>createProfile(profile: Profile)\n</code></pre> Description Method to create a new profile."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelmeth-get-all-profiles","title":"@label(meth) Get All Profiles","text":"<pre><code>getProfiles(): BehaviorSubject&lt;Profile[]&gt;\n</code></pre> Description Method to get all existing profiles in the database store."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelmeth-get-specific-profiles","title":"@label(meth) Get Specific Profiles","text":"<pre><code>getProfile(profileId: string): BehaviorSubject&lt;Profile | undefined&gt;\n</code></pre> Description Method to get a specific profiles in the database store. If the profile does not exist, it does not return anything."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelmeth-update-profile","title":"@label(meth) Update Profile","text":"<pre><code>updateProfile(updatedProfile: Profile)\n</code></pre> Description Method to update the profile in updatedProfile."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/profile-service/#labelmeth-delete-profile","title":"@label(meth) Delete Profile","text":"<pre><code>deleteProfile(id: string)\n</code></pre> Description Method to delete a profile by id."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/","title":"Voice Activity Detection @label(deprecated)","text":"<p>This service is used to handle Voice Activity Detection (VAD).</p> <p>The current implementation uses the browser's <code>SpeechRecognition</code> API to detect speech. Transcription is still handled by the backend due to the following reasons.</p> <ol> <li>Multi language support. The browser requires the user to specify the language; leaving this capability to the backend allows us to automatically detect the user's language.</li> <li>Limited browser support. Not all browsers support this API (e.g. Arc, Firefox), leaving this to our own API allows us to have full control and simplify the process of Speech to Text (STT) transcription.</li> </ol> <p>Fallback required</p> <p>A fall back VAD detector needs to be implemented in the event of an unsupported browser. This method can be done using live transcription from our backend, or using alternative methods (e.g. volume levels).</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#labelservice-vadservice","title":"@label(service) VadService","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#attributes","title":"Attributes","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#labelprivate-endtimeout","title":"@label(private) endTimeout","text":"<p><code>number</code> to keep track of timeout calls that are used in <code>VadService.start()</code> to emit an event when speech has ended.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#labelprivate-speech","title":"@label(private) $speech","text":"<p><code>Subject&lt;void&gt;</code> emits an event whenever speech has been detected; fired through a subscription in <code>VadService.configSpeechRecognition()</code> as a side effect.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#labelprivate-recognition","title":"@label(private) recognition","text":"<p><code>SpeechRecognition</code> object from the browser.</p>"},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#methods","title":"Methods","text":""},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#labelprivate-labelmeth-configure","title":"@label(private) @label(meth) Configure","text":"<pre><code>private configSpeechRecofnition(): void\n</code></pre> Description Method to instantiate the speech recognition object. Called in the constructor."},{"location":"projects/genai/conversational-assistant/webapp/frontend/services/voice-activity-detection/#labelmeth-start","title":"@label(meth) Start","text":"<pre><code>start(): Observable&lt;VoiceActivity&gt;\n</code></pre> Description Method to get an observable to keep track of VAD activity. Returns <code>Observable&lt;VoiceActivity&gt;</code>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/engineering/","title":"Engineering","text":""},{"location":"blog/category/guides/","title":"Guides","text":""}]}